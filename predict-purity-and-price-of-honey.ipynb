{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Purity and Price of Honey - Exploratory Data Analysis (EDA)\n",
    "#### Project Summary:\n",
    "This project deals with the Exploratory Data Analysis (EDA) process to understand the relationship between the purity and price of honey. The “Predict Purity and Price of Honey” dataset was used to analyze various properties of honey and their effects on purity and price. Inferences were made on various categorical and numerical variables in the dataset and relationships were examined through visualizations and statistical analysis.\n",
    "\n",
    "#### About Dataset:\n",
    "The dataset contains attributes that provide information about the purity and price of honey. The attributes include physical parameters such as pH, color, density. Studies have been conducted on how these parameters can affect the purity and price of honey.\n",
    "\n",
    "- **CS (Color Score):**\n",
    "    - Represents the color score of the honey sample, ranging from 1.0 to 10.0. Lower values indicate a lighter color, while higher values indicate a darker color.\n",
    "\n",
    "- **Density:**\n",
    "    - Represents the density of the honey sample in grams per cubic centimeter at 25°C, ranging from 1.21 to 1.86.\n",
    "\n",
    "- **WC (Water Content):**\n",
    "    - Represents the water content in the honey sample, ranging from 12.0% to 25.0%.\n",
    "\n",
    "- **pH:**\n",
    "    - Represents the pH level of the honey sample, ranging from 2.50 to 7.50.\n",
    "\n",
    "- **EC (Electrical Conductivity):**\n",
    "    - Represents the electrical conductivity of the honey sample in milliSiemens per centimeter.\n",
    "\n",
    "- **F (Fructose Level):**\n",
    "    - Represents the fructose level of the honey sample, ranging from 20 to 50.\n",
    "\n",
    "- **G (Glucose Level):**\n",
    "    - Represents the glucose level of the honey sample, ranging from 20 to 45.\n",
    "\n",
    "- **Pollen_analysis:**\n",
    "    - Represents the floral source of the honey sample. Possible values include Clover, Wildflower, Orange Blossom, Alfalfa, Acacia, Lavender, Eucalyptus, Buckwheat, Manuka, Sage, Sunflower, Borage, Rosemary, Thyme, Heather, Tupelo, Blueberry, Chestnut, and Avocado.\n",
    "\n",
    "- **Viscosity:**\n",
    "    - Represents the viscosity of the honey sample in centipoise, ranging from 1500 to 10000. Viscosity values between 2500 and 9500 are considered optimal for purity.\n",
    "\n",
    "- **Purity:**\n",
    "    - The target variable represents the purity of the honey sample, ranging from 0.01 to 1.00.\n",
    "\n",
    "- **Price:**\n",
    "    - The calculated price of the honey.\n",
    "\n",
    "\n",
    "### Objective:\n",
    "The main objective in this project is to analyze the relationship between the purity levels and the price of honey and determine which factors have more influence on these variables. Furthermore, this EDA process aims to uncover missing data and potential anomalies in the dataset.\n",
    "\n",
    "#### Studies Conducted:\n",
    "- #### Data Cleaning and Preprocessing:\n",
    "\n",
    "    - Missing data checking and cleaning.\n",
    "    - Organizing and converting categorical data into numerical data.\n",
    "    - Analysis of outliers in data.\n",
    "- ### Exploratory Data Analysis (EDA):\n",
    "    \n",
    "    - Extraction of basic statistics from the data set (mean, median, standard deviation, etc.).\n",
    "    - Visualization of categorical data (pH levels, purity groups, etc.).\n",
    "    - Correlation analysis of numerical data and examination of their distributions.\n",
    "    - Comparison of purity distribution according to different pH groups.\n",
    "- ### Visualizations:\n",
    "\n",
    "    - Visualization of distributions and relationships of variables using graphs such as box plots, histograms and scatter plots.\n",
    "    - Visual analysis of categorical variables using pie charts and bar charts.\n",
    "- #### Statistical Tests:\n",
    "\n",
    "    - ANOVA test to compare purity averages across pH groups.\n",
    "    - Analyzing the effects of different pH ranges on the purity and price of honey.\n",
    "- #### Results:\n",
    "    - A significant relationship was observed between pH value and purity.\n",
    "    - When the purity averages of honey in different pH groups were compared, it was found that some pH ranges had higher purity levels than others.\n",
    "    - There was a linear relationship between price and purity, with higher purity honey generally being more expensive.\n",
    "- ### Conclusions:\n",
    "    This EDA process provides valuable insights into understanding the relationship of honey purity and price with certain characteristics. In particular, the effects of pH and other physical properties on honey purity and price provide important decision support tools for honey producers and consumers.\n",
    "\n",
    "### Next Steps:\n",
    "This analysis can serve as a basis for further modeling studies (e.g., linear regression or machine learning algorithms) and predictive analyses to estimate honey purity. Furthermore, more sophisticated trait engineering and modeling methods could be applied for honey price prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: auto; height: auto; overflow: hidden; text-align: center;\">\n",
    "  <img src=\"https://png.pngtree.com/thumb_back/fw800/background/20240716/pngtree-on-a-white-background-honeycomb-with-honey-drops-image_16002385.jpg\" alt=\"Melting Honey\" style=\"width: 100%; height: 100%; object-fit: cover;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:02.964302Z",
     "iopub.status.busy": "2025-04-11T06:35:02.963892Z",
     "iopub.status.idle": "2025-04-11T06:35:04.583814Z",
     "shell.execute_reply": "2025-04-11T06:35:04.582883Z",
     "shell.execute_reply.started": "2025-04-11T06:35:02.964251Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Basic Libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m \u001b[38;5;66;03m# Data Manipulation\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \u001b[38;5;66;03m# Mathematical Operations\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Data Virtualization\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd # Data Manipulation\n",
    "import numpy as np # Mathematical Operations\n",
    "\n",
    "\n",
    "# Data Virtualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# statistic\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:04.587184Z",
     "iopub.status.busy": "2025-04-11T06:35:04.585759Z",
     "iopub.status.idle": "2025-04-11T06:35:04.592393Z",
     "shell.execute_reply": "2025-04-11T06:35:04.591273Z",
     "shell.execute_reply.started": "2025-04-11T06:35:04.587133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_global_random_seed(seed: int = 42):\n",
    "    \"\"\"Sets a global random seed for reproducibility across various libraries.\"\"\"\n",
    "    # Set Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Seed Ayarlama\n",
    "set_global_random_seed(seed=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:04.593992Z",
     "iopub.status.busy": "2025-04-11T06:35:04.593619Z",
     "iopub.status.idle": "2025-04-11T06:35:05.424344Z",
     "shell.execute_reply": "2025-04-11T06:35:05.423056Z",
     "shell.execute_reply.started": "2025-04-11T06:35:04.593962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/predict-purity-and-price-of-honey/honey_purity_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Look at the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.427360Z",
     "iopub.status.busy": "2025-04-11T06:35:05.426979Z",
     "iopub.status.idle": "2025-04-11T06:35:05.608920Z",
     "shell.execute_reply": "2025-04-11T06:35:05.607862Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.427324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_random_missing_values(dataframe: pd.DataFrame,\n",
    "                              missing_rate: float = 0.05,\n",
    "                              random_state: int = 42,\n",
    "                              exclude_columns: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Turns random values to NaN in a DataFrame with reproducibility, excluding specified columns.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame to be processed.\n",
    "        missing_rate (float): Percentage of missing value rate in float format. Defaults 0.05.\n",
    "        random_state (int): Seed for random number generator. Defaults 42.\n",
    "        exclude_columns (list): List of column names to exclude from missing value assignment.\n",
    "\n",
    "    Returns:\n",
    "        df_missing (pd.DataFrame): Processed DataFrame object.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Get copy of dataframe\n",
    "    df_missing = dataframe.copy()\n",
    "\n",
    "    # Determine columns to include\n",
    "    if exclude_columns:\n",
    "        columns_to_include = [col for col in dataframe.columns if col not in exclude_columns]\n",
    "    else:\n",
    "        columns_to_include = dataframe.columns.tolist()\n",
    "\n",
    "    # Ensure we have at least one column to modify\n",
    "    if not columns_to_include:\n",
    "        raise ValueError(\"All columns are excluded. No missing values can be added.\")\n",
    "\n",
    "    # Obtain size of dataframe and number of total missing values\n",
    "    df_size = len(columns_to_include) * dataframe.shape[0]\n",
    "    num_missing = int(df_size * missing_rate)\n",
    "\n",
    "    # Map included column indices\n",
    "    included_col_indices = [dataframe.columns.get_loc(col) for col in columns_to_include]\n",
    "\n",
    "    # Generate random row and column indexes\n",
    "    row_indices = np.random.randint(0, dataframe.shape[0], num_missing)\n",
    "    col_indices = np.random.choice(included_col_indices, num_missing)\n",
    "\n",
    "    # Turn selected values into NaN\n",
    "    for row_idx, col_idx in zip(row_indices, col_indices):\n",
    "        df_missing.iat[row_idx, col_idx] = np.nan\n",
    "\n",
    "    return df_missing\n",
    "\n",
    "\n",
    "# Veri çerçevesine %0.05 NaN ekle\n",
    "df = add_random_missing_values(data, missing_rate=0.005,random_state=42,\n",
    "                               exclude_columns=[\"Pollen_analysis\",\"Price\"])\n",
    "df.head() # Gets the first 5 rows of the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.610811Z",
     "iopub.status.busy": "2025-04-11T06:35:05.610267Z",
     "iopub.status.idle": "2025-04-11T06:35:05.627978Z",
     "shell.execute_reply": "2025-04-11T06:35:05.626876Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.610774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.tail() # Gets the last 5 rows of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.629709Z",
     "iopub.status.busy": "2025-04-11T06:35:05.629308Z",
     "iopub.status.idle": "2025-04-11T06:35:05.664151Z",
     "shell.execute_reply": "2025-04-11T06:35:05.663009Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.629674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.sample(10) # Gets the random 10 rows of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we first look at the data set, we see that the columns except the **Pollen_analysis** column contain numeric values.\n",
    "* We see that there are missing values in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.666175Z",
     "iopub.status.busy": "2025-04-11T06:35:05.665754Z",
     "iopub.status.idle": "2025-04-11T06:35:05.672349Z",
     "shell.execute_reply": "2025-04-11T06:35:05.671299Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.666131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Veri Setinin Boyutu: \", df.shape) # row,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.673978Z",
     "iopub.status.busy": "2025-04-11T06:35:05.673658Z",
     "iopub.status.idle": "2025-04-11T06:35:05.717608Z",
     "shell.execute_reply": "2025-04-11T06:35:05.716220Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.673946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info() # Summary about the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **isna()** method checks whether the objects of a Data Frame or Series contain missing or null values (NA, NaN) and returns a new object with the same shape as the original, but whose elements have boolean values True or False. True indicates the presence of null or missing values, False indicates otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.719317Z",
     "iopub.status.busy": "2025-04-11T06:35:05.718984Z",
     "iopub.status.idle": "2025-04-11T06:35:05.752275Z",
     "shell.execute_reply": "2025-04-11T06:35:05.750855Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.719271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame showing the missing data\n",
    "missing = pd.DataFrame(df.isna().sum()).rename(columns={0:\"Miss_values\"}) \n",
    "# Create Missing Percent Column\n",
    "missing[\"Miss_Percent\"] = missing[\"Miss_values\"] / len(df)\n",
    "\n",
    "missing.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.757316Z",
     "iopub.status.busy": "2025-04-11T06:35:05.756900Z",
     "iopub.status.idle": "2025-04-11T06:35:05.769438Z",
     "shell.execute_reply": "2025-04-11T06:35:05.768035Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.757280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DataFrame with only missing values\n",
    "only_miss_val = missing[missing[\"Miss_values\"]> 0]\n",
    "only_miss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:05.771327Z",
     "iopub.status.busy": "2025-04-11T06:35:05.770867Z",
     "iopub.status.idle": "2025-04-11T06:35:06.108839Z",
     "shell.execute_reply": "2025-04-11T06:35:06.107687Z",
     "shell.execute_reply.started": "2025-04-11T06:35:05.771290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data = only_miss_val , y=only_miss_val.index, x=\"Miss_values\")\n",
    "plt.title(\"Missing Values with Barplot\")\n",
    "plt.xlabel(\"Number of Missing Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any duplicate records?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **duplicated()** method returns a Series with True and False values that define which rows in the DataFrame are duplicated and which are not. Use the subset parameter to specify which columns to include when searching for duplicates. By default all columns are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.110438Z",
     "iopub.status.busy": "2025-04-11T06:35:06.110094Z",
     "iopub.status.idle": "2025-04-11T06:35:06.263630Z",
     "shell.execute_reply": "2025-04-11T06:35:06.262048Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.110385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Number of duplicate records\n",
    "print(\"Number of duplicate records in the dataset: \",df.duplicated().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Values for Each Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **unique()** method returns the sorted unique elements of an array. In addition to the unique elements there are three optional outputs: \n",
    "* The indices of the input array that yields the unique values\n",
    "* The indices of the unique array that reconstructs the input array\n",
    "* The number of times each unique value occurs in the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.265924Z",
     "iopub.status.busy": "2025-04-11T06:35:06.265254Z",
     "iopub.status.idle": "2025-04-11T06:35:06.342151Z",
     "shell.execute_reply": "2025-04-11T06:35:06.341110Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.265872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def unique_values(df:pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Calculates and prints the number of unique values in each column of a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame for which the unique values will be calculated.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        This function does not return anything; it prints the number of unique values \n",
    "        for each column.\n",
    "\n",
    "    \"\"\"\n",
    "    unique_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        number_of_unique = df[col].nunique()\n",
    "        print(f\"Number of unique values in column {col}: {number_of_unique}\")\n",
    "        print(\"==\"*22)\n",
    "unique_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.344526Z",
     "iopub.status.busy": "2025-04-11T06:35:06.344052Z",
     "iopub.status.idle": "2025-04-11T06:35:06.354201Z",
     "shell.execute_reply": "2025-04-11T06:35:06.353144Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.344478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Summary Function\n",
    "def get_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a summary of the given DataFrame, including descriptive statistics and additional information.\n",
    "\n",
    "    This function returns a summary DataFrame with the following columns for each feature:\n",
    "    - Count: Non-missing entries in each column.\n",
    "    - unique: Number of unique values in each column.\n",
    "    - missing: Number of missing values in each column.\n",
    "    - duplicated: Total number of duplicate rows in the DataFrame.\n",
    "    - mean: Mean of the column (for numerical columns only).\n",
    "    - std: Standard deviation of the column (for numerical columns only).\n",
    "    - min: Minimum value in the column (for numerical columns only).\n",
    "    - 25%: 25th percentile of the column (for numerical columns only).\n",
    "    - 50%: 50th percentile (median) of the column (for numerical columns only).\n",
    "    - 75%: 75th percentile of the column (for numerical columns only).\n",
    "    - max: Maximum value in the column (for numerical columns only).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A styled DataFrame with descriptive statistics and additional information,\n",
    "                      including background color gradients and borders for visual emphasis.\n",
    "    \"\"\"\n",
    "    df_desc = pd.DataFrame(df.describe(include=\"all\").T)\n",
    "    df_summary = pd.DataFrame({\"dtype\": df.dtypes,\n",
    "                               \"Count\": df.count(),\n",
    "                               \"unique\": df.nunique(),\n",
    "                               \"missing\": df.isna().sum(),\n",
    "                               \"duplicated\": df.duplicated().sum(),\n",
    "                               \"mean\": df_desc[\"mean\"].values,\n",
    "                               \"min\": df_desc[\"min\"].values,\n",
    "                               \"std\": df_desc[\"std\"].values,\n",
    "                               \"25%\": df_desc[\"25%\"].values,\n",
    "                               \"50%\": df_desc[\"50%\"].values,\n",
    "                               \"75%\": df_desc[\"75%\"].values,\n",
    "                               \"max\": df_desc[\"max\"].values})\n",
    "\n",
    "    return df_summary.style\\\n",
    "        .background_gradient(cmap='YlGnBu', subset=['mean', 'std', 'min', '25%', '50%', '75%', 'max'])\\\n",
    "        .set_properties(**{'border': '1.5px solid black'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.355845Z",
     "iopub.status.busy": "2025-04-11T06:35:06.355505Z",
     "iopub.status.idle": "2025-04-11T06:35:06.815978Z",
     "shell.execute_reply": "2025-04-11T06:35:06.814960Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.355815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "get_summary(df) # Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset Description Based on First Step:**\n",
    "Below is a detailed description of each column in the dataset:\n",
    "\n",
    "1. **CS**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the color score of the honey sample. Lower values indicate a lighter color, while higher values indicate a darker color.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 5.49\n",
    "     - **Standard Deviation**: 2.59\n",
    "     - **Minimum**: 1.00\n",
    "     - **25th Percentile**: 3.26\n",
    "     - **Median**: 5.50\n",
    "     - **75th Percentile**: 7.74\n",
    "     - **Maximum**: 10\n",
    "2. **Density**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the density of the honey sample in grams per cubic centimeter at 25°C.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 1.53\n",
    "     - **Standard Deviation**: 0.18\n",
    "     - **Minimum**: 1.21\n",
    "     - **25th Percentile**: 1.37\n",
    "     - **Median**: 1.54\n",
    "     - **75th Percentile**: 1.70\n",
    "     - **Maximum**: 1.86\n",
    "3. **WC**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the water content in the honey sample.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 18.50\n",
    "     - **Standard Deviation**: 3.74\n",
    "     - **Minimum**: 12\n",
    "     - **25th Percentile**: 15.26\n",
    "     - **Median**: 18.51\n",
    "     - **75th Percentile**: 21.75\n",
    "     - **Maximum**: 25\n",
    "4. **pH**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the pH level of the honey sample.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 4.99\n",
    "     - **Standard Deviation**: 1.44\n",
    "     - **Minimum**: 2.5\n",
    "     - **25th Percentile**: 3.74\n",
    "     - **Median**: 4.99\n",
    "     - **75th Percentile**: 6.25\n",
    "     - **Maximum**: 7.50 \n",
    "5. **EC**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the electrical conductivity of the honey sample in milliSiemens per centimeter.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 0.79\n",
    "     - **Standard Deviation**: 0.05\n",
    "     - **Minimum**: 0.70\n",
    "     - **25th Percentile**: 0.75\n",
    "     - **Median**: 0.80\n",
    "     - **75th Percentile**: 0.85\n",
    "     - **Maximum**: 0.90\n",
    "6. **F**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the fructose level of the honey sample.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 34.97\n",
    "     - **Standard Deviation**: 8.65\n",
    "     - **Minimum**: 20.00\n",
    "     - **25th Percentile**: 27.46\n",
    "     - **Median**: 34.97\n",
    "     - **75th Percentile**: 44.47\n",
    "     - **Maximum**: 50.00   \n",
    "7. **G**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the glucose level of the honey sample.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 32.50\n",
    "     - **Standard Deviation**: 7.22\n",
    "     - **Minimum**: 20.00\n",
    "     - **25th Percentile**: 26.23\n",
    "     - **Median**: 32.50\n",
    "     - **75th Percentile**: 38.76\n",
    "     - **Maximum**: 45.00\n",
    "8. **Pollen_analysis**\n",
    "   - **Type**: Categorical\n",
    "   - **Description**: Represents the floral source of the honey sample.\n",
    "\n",
    "9. **Viscosity**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the viscosity of the honey sample in centipoise.Viscosity values between 2500 and 9500 are considered optimal for purity.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 5752.32\n",
    "     - **Standard Deviation**: 2455.72\n",
    "     - **Minimum**: 1500.05\n",
    "     - **25th Percentile**: 3627.17\n",
    "     - **Median**: 5752.66\n",
    "     - **75th Percentile**: 7886.04\n",
    "     - **Maximum**: 9999.97\n",
    "10. **Purity**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the viscosity of the honey sample in centipoise.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 0.82\n",
    "     - **Standard Deviation**: 0.13\n",
    "     - **Minimum**: 0.61\n",
    "     - **25th Percentile**: 0.66\n",
    "     - **Median**: 0.82\n",
    "     - **75th Percentile**: 0.97\n",
    "     - **Maximum**: 1.00\n",
    "11. **Price**\n",
    "   - **Type**: Numerical\n",
    "   - **Description**: Represents the viscosity of the honey sample in centipoise.\n",
    "   - **Summary Statistics**:\n",
    "     - **Mean**: 594.80\n",
    "     - **Standard Deviation**: 233.62\n",
    "     - **Minimum**: 128.72\n",
    "     - **25th Percentile**: 433.00\n",
    "     - **Median**: 612.96\n",
    "     - **75th Percentile**: 770.22\n",
    "     - **Maximum**: 976.69\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of Categorical and Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.817680Z",
     "iopub.status.busy": "2025-04-11T06:35:06.817183Z",
     "iopub.status.idle": "2025-04-11T06:35:06.824937Z",
     "shell.execute_reply": "2025-04-11T06:35:06.823900Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.817643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List Comprehension for numeric features\n",
    "numerical_columns = [col for col in df.columns if df[col].dtypes in [\"float\",\"int\"]]\n",
    "# List Comprehension for categoric features\n",
    "categorical_columns = [col for col in df.columns if df[col].dtypes in [\"object\",\"category\"]]\n",
    "\n",
    "print(f\"Number of columns containing numeric values: {len(numerical_columns)}\")\n",
    "print(f\"Columns containing numeric values: {numerical_columns}\")\n",
    "print()\n",
    "print(f\"Number of columns containing categorical values: {len(categorical_columns)}\")\n",
    "print(f\"Columns containing categorical values: {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Distribution\n",
    "First, let's look at the distribution of the target variable. For this, we will use the **histogram** graph.\n",
    "\n",
    "We will also use **boxplot** to see the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:06.826934Z",
     "iopub.status.busy": "2025-04-11T06:35:06.826268Z",
     "iopub.status.idle": "2025-04-11T06:35:08.210332Z",
     "shell.execute_reply": "2025-04-11T06:35:08.209301Z",
     "shell.execute_reply.started": "2025-04-11T06:35:06.826897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Add axes for boxplot\n",
    "box_axes = fig.add_axes([0.1, 0.1, 0.35, 0.8])  # [left, bottom, width, height]\n",
    "sns.kdeplot(x=df[\"Price\"], color='blue', ax=box_axes)\n",
    "box_axes.set_title('KDE Plot', fontsize=14)\n",
    "box_axes.set_xlabel('Price', fontsize=12)\n",
    "\n",
    "# Add axes for histogram with KDE\n",
    "hist_axes = fig.add_axes([0.55, 0.1, 0.35, 0.8])  # [left, bottom, width, height]\n",
    "sns.distplot(\n",
    "    x=df[\"Price\"],kde=False,\n",
    "    color='#967bb6', ax=hist_axes,\n",
    ")\n",
    "hist_axes.set_title('Distribution', fontsize=14)\n",
    "hist_axes.set_xlabel('Price', fontsize=12)\n",
    "hist_axes.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Implications of the Price Histogram:**\n",
    "- **Price Distribution:**\n",
    "\n",
    "    - The histogram clearly shows how honey is distributed across price ranges. This determines which price ranges are prevalent in the market.\n",
    "    - For example, a large peak around 600 on the chart may indicate that honey is most commonly sold or priced within that price range.\n",
    "\n",
    "- **Segmentation:**\n",
    "\n",
    "    -  Multiple peaks (mode) may indicate that honey products are divided into price segments\n",
    "    - Cheap (for example, low quality or additive-containing products),\n",
    "    - Mid-priced (more common quality products),\n",
    "    - Premium (high quality or natural honey)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation \n",
    "**Correlation matrix** is a statistical technique used to evaluate the relationship between two variables in a data set. The matrix is ​​a table in which each cell contains a correlation coefficient, where **1** is considered a strong relationship between the variables, **0** is considered a neutral relationship, and **-1** is considered a weak relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:08.211956Z",
     "iopub.status.busy": "2025-04-11T06:35:08.211648Z",
     "iopub.status.idle": "2025-04-11T06:35:08.295635Z",
     "shell.execute_reply": "2025-04-11T06:35:08.294501Z",
     "shell.execute_reply.started": "2025-04-11T06:35:08.211925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Shows the degree to which the Price value is linearly related to other features\n",
    "df.corr(numeric_only=True)[\"Price\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:08.297492Z",
     "iopub.status.busy": "2025-04-11T06:35:08.297038Z",
     "iopub.status.idle": "2025-04-11T06:35:08.945265Z",
     "shell.execute_reply": "2025-04-11T06:35:08.942546Z",
     "shell.execute_reply.started": "2025-04-11T06:35:08.297445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    mask=mask, \n",
    "    annot=True, # Show correlation values\n",
    "    fmt=\".3f\", # Value format\n",
    "    cmap='coolwarm', # Color scale\n",
    "    vmin=-1, vmax=1, # Correlation range\n",
    "    linewidths=0.5, # Cell separator width\n",
    "    cbar_kws={'shrink': 0.8} # Color bar size\n",
    "    )\n",
    "\n",
    "plt.title('Correlation Matrix (Upper Triangle Hidden)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pollen_analysis** is highly overall correlated with **Price**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity ve Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:08.947570Z",
     "iopub.status.busy": "2025-04-11T06:35:08.947105Z",
     "iopub.status.idle": "2025-04-11T06:35:15.114257Z",
     "shell.execute_reply": "2025-04-11T06:35:15.112860Z",
     "shell.execute_reply.started": "2025-04-11T06:35:08.947521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(df,x=\"Purity\",y = \"Price\",hue=\"Pollen_analysis\",palette=\"tab10\")\n",
    "plt.title(\"Scatter by Purity & Price\") \n",
    "plt.xlabel(\"Purity\") \n",
    "plt.ylabel(\"Price\") \n",
    "# Set Legend position to top left\n",
    "plt.legend(loc=[0.23, 0.295], title=\"Pollen Analysis\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:15.115880Z",
     "iopub.status.busy": "2025-04-11T06:35:15.115547Z",
     "iopub.status.idle": "2025-04-11T06:35:15.143038Z",
     "shell.execute_reply": "2025-04-11T06:35:15.142041Z",
     "shell.execute_reply.started": "2025-04-11T06:35:15.115848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "group_df = df.copy()\n",
    "# Segmentation according to purity values\n",
    "group_df['Purity_group'] = pd.cut(group_df['Purity'], bins=[0.6, 0.8, 0.9, 1.0], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Average pH value for each group\n",
    "purity_group_means = group_df.groupby('Purity_group')['Price'].mean()\n",
    "\n",
    "print(purity_group_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:15.144572Z",
     "iopub.status.busy": "2025-04-11T06:35:15.144247Z",
     "iopub.status.idle": "2025-04-11T06:35:15.326968Z",
     "shell.execute_reply": "2025-04-11T06:35:15.325794Z",
     "shell.execute_reply.started": "2025-04-11T06:35:15.144541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize Price averages with bar plot\n",
    "purity_group_means.plot(kind='bar')\n",
    "plt.title('Purity Gruplarına Göre Price Ortalamaları')\n",
    "plt.ylabel('Price Ortalaması')\n",
    "plt.xlabel('Purity Grubu')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:15.329001Z",
     "iopub.status.busy": "2025-04-11T06:35:15.328527Z",
     "iopub.status.idle": "2025-04-11T06:35:15.424820Z",
     "shell.execute_reply": "2025-04-11T06:35:15.423913Z",
     "shell.execute_reply.started": "2025-04-11T06:35:15.328954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Counting by purity groups\n",
    "purity_group_counts = group_df['Purity_group'].value_counts()\n",
    "\n",
    "# Create Pie plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.pie(purity_group_counts, labels=purity_group_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "plt.title('Purity Grubuna Göre Dağılım')\n",
    "plt.axis('equal') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:15.426390Z",
     "iopub.status.busy": "2025-04-11T06:35:15.425979Z",
     "iopub.status.idle": "2025-04-11T06:35:15.531729Z",
     "shell.execute_reply": "2025-04-11T06:35:15.530821Z",
     "shell.execute_reply.started": "2025-04-11T06:35:15.426347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.pie(purity_group_means, labels=purity_group_means.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "plt.title('Purity Grubuna Göre Ortalama Price Dağılımı')\n",
    "plt.axis('equal') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation, Target Variable, and Insights from the Purity Distribution of the Target Variable:\n",
    "\n",
    "#### 1. Honey Purity and Price Distribution\n",
    "* **Multimodal Distribution:** Multiple peaks (modes) in the graph can indicate different groups in the honey purity or price:\n",
    "   - This could suggest honey groups with varying purity levels (e.g., 100% natural, adulterated, low quality).\n",
    "   - It might also reflect different price ranges (low-priced, mid-priced, premium).\n",
    "     - For example: The pronounced peak around 600 could indicate a common honey type in this price range, whether it’s pure or widespread.\n",
    "\n",
    "#### 2. Density and Segments\n",
    "   The presence of peak points in different regions suggests clear segmentation in honey purity and price:\n",
    "   - **Low-Priced and Low Purity:** If concentration is observed in lower ranges, it may represent honey groups with added substances.\n",
    "   - **High-Priced and High Purity:** Peaks in higher price ranges might indicate premium and natural honey products.\n",
    "\n",
    "#### 3. Distribution Range\n",
    "   - The data spreads across a broad range (from 200 to 1000). This implies significant variability in honey prices and purity levels.\n",
    "   - Possible explanations:\n",
    "     - Different honey types (e.g., flower honey, chestnut honey, multifloral honey) or production methods (organic, conventional) could contribute to this variability.\n",
    "     - Regional differences (e.g., imported honey vs. local honey) might cause such diversity.\n",
    "\n",
    "#### 4. Anomalies\n",
    "   - **Concentration Ranges:** The clear concentration around 600 suggests that honey in this range is widespread in terms of price/purity. This could indicate that the data collection process focused on a specific segment of the market.\n",
    "   - **Possible Outliers:** Irregularities in the KDE curve may point to anomalies or unexpected behaviors.\n",
    "     - For example: Honey with very low purity but high prices may represent cases where the price doesn’t reflect the quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pH ve Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:15.533253Z",
     "iopub.status.busy": "2025-04-11T06:35:15.532849Z",
     "iopub.status.idle": "2025-04-11T06:35:16.307503Z",
     "shell.execute_reply": "2025-04-11T06:35:16.306529Z",
     "shell.execute_reply.started": "2025-04-11T06:35:15.533206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(df,x=\"Purity\",y = \"pH\")\n",
    "plt.title(\"Scatter by Purity & Price\") \n",
    "plt.xlabel(\"Purity\") \n",
    "plt.ylabel(\"pH\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Research Questions:**\n",
    "\n",
    "- Why are purity levels clustered at regular pH values?\n",
    "- Is there a direct impact of pH on safety?\n",
    "- Are higher purity levels consistent with specific pH values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.309006Z",
     "iopub.status.busy": "2025-04-11T06:35:16.308722Z",
     "iopub.status.idle": "2025-04-11T06:35:16.326643Z",
     "shell.execute_reply": "2025-04-11T06:35:16.325612Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.308978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Segmentation according to pH values\n",
    "group_df['pH_group'] = pd.cut(group_df['pH'], bins=[2, 4, 6, 8], labels=['Low', 'Medium', 'Normal'])\n",
    "\n",
    "# Average purity for each group\n",
    "group_means = group_df.groupby('pH_group')['Purity'].mean()\n",
    "print(group_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.328068Z",
     "iopub.status.busy": "2025-04-11T06:35:16.327771Z",
     "iopub.status.idle": "2025-04-11T06:35:16.557873Z",
     "shell.execute_reply": "2025-04-11T06:35:16.556753Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.328039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "group_means.plot(kind='bar')\n",
    "plt.title('pH Gruplarına Göre Purity Ortalamaları')\n",
    "plt.ylabel('pH Ortalaması')\n",
    "plt.xlabel('Purity Grubu')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.567050Z",
     "iopub.status.busy": "2025-04-11T06:35:16.566633Z",
     "iopub.status.idle": "2025-04-11T06:35:16.678589Z",
     "shell.execute_reply": "2025-04-11T06:35:16.677498Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.567014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Counting by purity groups\n",
    "ph_group_counts = group_df['pH_group'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.pie(ph_group_counts, labels=ph_group_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "plt.title('pH Grubuna Göre Dağılım')\n",
    "plt.axis('equal')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.680139Z",
     "iopub.status.busy": "2025-04-11T06:35:16.679735Z",
     "iopub.status.idle": "2025-04-11T06:35:16.782684Z",
     "shell.execute_reply": "2025-04-11T06:35:16.781621Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.680093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.pie(group_means, labels=group_means.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "plt.title('pH Grubuna Göre Purity Dağılımı')\n",
    "plt.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_value < 0.05 olduğundan pH değeri farklı olan gruplar arasında saflık açısından anlamlı bir fark vardır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viscosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.784271Z",
     "iopub.status.busy": "2025-04-11T06:35:16.783895Z",
     "iopub.status.idle": "2025-04-11T06:35:16.808480Z",
     "shell.execute_reply": "2025-04-11T06:35:16.807384Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.784234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data set containing the optimal purity value\n",
    "vis_df = group_df.loc[(group_df[\"Viscosity\"] >= 2500) & (group_df[\"Viscosity\"] <= 10000)][\"Purity_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.810255Z",
     "iopub.status.busy": "2025-04-11T06:35:16.809847Z",
     "iopub.status.idle": "2025-04-11T06:35:16.974282Z",
     "shell.execute_reply": "2025-04-11T06:35:16.972112Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.810210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "vis_df.plot(kind=\"bar\",color=\"skyblue\")\n",
    "plt.xlabel(\"Purity Group\")\n",
    "plt.ylabel(\"Viscosity Count\")\n",
    "plt.title(\"Viscosity Number by Purity Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "\n",
    "Upon examining the dataset, it is mentioned that the optimal viscosity range is between 2500 and 10000. However, upon closer inspection, we observe records with high viscosity values but low purity values.\n",
    "\n",
    "#### Possible Causes and Comments:\n",
    "\n",
    "- **Additives or Processing Effects:**\n",
    "\n",
    "    - Additives (such as sugar syrup, starch, or gelatin) might have been added to increase the viscosity of the honey. This could reduce the purity of the honey.\n",
    "    - Honey that has undergone industrial processing, such as concentration, could increase its viscosity but lose its natural structure and purity.\n",
    "\n",
    "-  **Low Moisture Content:**\n",
    "\n",
    "      - The honey may have low moisture content. While low moisture can increase viscosity, it doesn't necessarily guarantee high purity. If purity is low, low moisture could just be a physical characteristic.\n",
    "\n",
    "-  **Non-Floral Properties:**\n",
    "\n",
    "      - Non-natural feeding conditions (e.g., feeding bees sugar syrup) or adulterated honey production can lead to high viscosity but low purity.\n",
    "\n",
    "-  **Carbohydrate Composition Changes:**\n",
    "\n",
    "      - Alterations in the glucose and fructose ratio in the honey, in unnatural proportions, might increase viscosity while decreasing purity.\n",
    "\n",
    "-   **Crystallization (Granulation):**\n",
    "\n",
    "      - The increase in viscosity might be due to crystallization. However, crystallization is a natural process. If purity is low, crystallization might be due to additives rather than natural contents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the **Mean** and **Median** values are very close to each other, we can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:16.976050Z",
     "iopub.status.busy": "2025-04-11T06:35:16.975629Z",
     "iopub.status.idle": "2025-04-11T06:35:17.407432Z",
     "shell.execute_reply": "2025-04-11T06:35:17.406555Z",
     "shell.execute_reply.started": "2025-04-11T06:35:16.976002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fill_df = df.copy()\n",
    "# Calculates the average purity values ​​for each pollen type\n",
    "purity_mean_df = df.groupby([\"Pollen_analysis\"])[\"Purity\"].mean().reset_index()\n",
    "\n",
    "# Filling in missing values with the average of purity values for each pollen type\n",
    "for row in zip(purity_mean_df[\"Pollen_analysis\"], purity_mean_df[\"Purity\"]):\n",
    "    fill_df.loc[((fill_df[\"Purity\"].isna()) & (fill_df[\"Pollen_analysis\"] == row[0])), \"Purity\"] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:17.409170Z",
     "iopub.status.busy": "2025-04-11T06:35:17.408767Z",
     "iopub.status.idle": "2025-04-11T06:35:17.441930Z",
     "shell.execute_reply": "2025-04-11T06:35:17.440581Z",
     "shell.execute_reply.started": "2025-04-11T06:35:17.409125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fill_group_df = fill_df.copy()\n",
    "fill_group_df['Purity_group'] = pd.cut(fill_group_df['Purity'], bins=[0.6, 0.8, 0.9, 1.0], labels=['Low', 'Medium', 'High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:17.443932Z",
     "iopub.status.busy": "2025-04-11T06:35:17.443378Z",
     "iopub.status.idle": "2025-04-11T06:35:18.703715Z",
     "shell.execute_reply": "2025-04-11T06:35:18.702651Z",
     "shell.execute_reply.started": "2025-04-11T06:35:17.443881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculates the average pH values for each pollen type and purity group\n",
    "ph_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"pH\"].mean().reset_index()\n",
    "\n",
    "\n",
    "# Filling in missing values with the average of pH values for each pollen type and purity group\n",
    "for row in zip(ph_mean_df[\"Pollen_analysis\"], ph_mean_df[\"Purity_group\"],ph_mean_df[\"pH\"]):\n",
    "    fill_group_df.loc[((fill_group_df[\"pH\"].isna()) & (fill_group_df[\"Pollen_analysis\"] == row[0]) & \n",
    "                       (fill_group_df[\"Purity_group\"]==row[1])), \"pH\"] = row[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:18.705704Z",
     "iopub.status.busy": "2025-04-11T06:35:18.705227Z",
     "iopub.status.idle": "2025-04-11T06:35:18.718271Z",
     "shell.execute_reply": "2025-04-11T06:35:18.717189Z",
     "shell.execute_reply.started": "2025-04-11T06:35:18.705645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fill_group_df['pH_group'] = pd.cut(fill_group_df['pH'], bins=[2, 4, 6, 8], labels=['Low', 'Medium', 'Normal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viscosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:18.720443Z",
     "iopub.status.busy": "2025-04-11T06:35:18.720016Z",
     "iopub.status.idle": "2025-04-11T06:35:19.998074Z",
     "shell.execute_reply": "2025-04-11T06:35:19.996991Z",
     "shell.execute_reply.started": "2025-04-11T06:35:18.720382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vis_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"Viscosity\"].mean().reset_index()\n",
    "\n",
    "for row in zip(vis_mean_df[\"Pollen_analysis\"], vis_mean_df[\"Purity_group\"],vis_mean_df[\"Viscosity\"]):\n",
    "    fill_group_df.loc[((fill_group_df[\"Viscosity\"].isna()) & (fill_group_df[\"Pollen_analysis\"] == row[0]) & \n",
    "                       (fill_group_df[\"Purity_group\"]==row[1])), \"Viscosity\"] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:19.999641Z",
     "iopub.status.busy": "2025-04-11T06:35:19.999316Z",
     "iopub.status.idle": "2025-04-11T06:35:20.008228Z",
     "shell.execute_reply": "2025-04-11T06:35:20.007055Z",
     "shell.execute_reply.started": "2025-04-11T06:35:19.999611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fill_value(df,group_df):\n",
    "    \"\"\"\n",
    "    Fills missing values in the target dataframe (df) based on a reference dataframe (group_df).\n",
    "\n",
    "    This function checks for missing values in the specified columns of the target dataframe \n",
    "    (df) and fills those missing values with corresponding values from the group dataframe \n",
    "    (group_df) based on certain column conditions.\n",
    "\n",
    "    The function supports two scenarios:\n",
    "    1. When `group_df` has 3 columns: it uses the first two columns to match the values and \n",
    "       fills the missing values in the third column.\n",
    "    2. When `group_df` has 2 columns: it uses the first column to match the values and \n",
    "       fills the missing values in the second column.\n",
    "\n",
    "    Args:\n",
    "    - df (pandas.DataFrame): The target dataframe containing missing values that need to be filled.\n",
    "    - group_df (pandas.DataFrame): The reference dataframe used to fill missing values in `df`.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A new dataframe (a copy of `df`) with the missing values filled.\n",
    "    \n",
    "    Example:\n",
    "    ```python\n",
    "    df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [None, 5, None, 7]})\n",
    "    group_df = pd.DataFrame({'A': [1, 3], 'B': [10, 30]})\n",
    "    filled_df = fill_value(df, group_df)\n",
    "    ```\n",
    "    In this example, missing values in column 'B' of `df` will be filled based on matching \n",
    "    values in column 'A' from `group_df`.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    columns_list = group_df.columns.to_list()\n",
    "    if len(columns_list) ==3:\n",
    "        for row in zip(group_df[columns_list[0]], group_df[columns_list[1]],group_df[columns_list[2]]):\n",
    "            df.loc[((df[columns_list[2]].isna()) & (df[columns_list[0]] == row[0]) & \n",
    "                               (df[columns_list[1]]==row[1])), columns_list[2]] = row[2]\n",
    "    elif len(columns_list) == 2:\n",
    "        for row in zip(group_df[columns_list[0]], group_df[columns_list[1]]):\n",
    "            df.loc[((df[columns_list[1]].isna()) & (df[columns_list[0]] == row[0])), columns_list[1]] = row[1]\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:20.009917Z",
     "iopub.status.busy": "2025-04-11T06:35:20.009569Z",
     "iopub.status.idle": "2025-04-11T06:35:20.043897Z",
     "shell.execute_reply": "2025-04-11T06:35:20.042809Z",
     "shell.execute_reply.started": "2025-04-11T06:35:20.009886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fill_group_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:20.045444Z",
     "iopub.status.busy": "2025-04-11T06:35:20.045111Z",
     "iopub.status.idle": "2025-04-11T06:35:27.491410Z",
     "shell.execute_reply": "2025-04-11T06:35:27.490468Z",
     "shell.execute_reply.started": "2025-04-11T06:35:20.045393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "g_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"G\"].mean().reset_index()\n",
    "f_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"F\"].mean().reset_index()\n",
    "ec_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"EC\"].mean().reset_index()\n",
    "wc_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"WC\"].mean().reset_index()\n",
    "cs_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"CS\"].mean().reset_index()\n",
    "dens_mean_df = fill_group_df.groupby([\"Pollen_analysis\",\"Purity_group\"])[\"Density\"].mean().reset_index()\n",
    "\n",
    "\n",
    "liste  = [dens_mean_df,cs_mean_df,wc_mean_df,ec_mean_df,f_mean_df,g_mean_df]\n",
    "for li in liste:\n",
    "    fill_value(fill_group_df, li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Frequency Table and Mode for Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:27.492888Z",
     "iopub.status.busy": "2025-04-11T06:35:27.492592Z",
     "iopub.status.idle": "2025-04-11T06:35:27.524181Z",
     "shell.execute_reply": "2025-04-11T06:35:27.523090Z",
     "shell.execute_reply.started": "2025-04-11T06:35:27.492858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List Comprehension for categoric features\n",
    "categorical_columns = [col for col in fill_group_df.columns if fill_group_df[col].dtypes in [\"object\",\"category\"]]\n",
    "\n",
    "# Calculate percentages for categorical columns\n",
    "category_percentages = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    category_percentages[col] = fill_group_df[col].value_counts(normalize=True) * 100\n",
    "\n",
    "for col, percentages in category_percentages.items():\n",
    "    print(f\"Percentages for {col}:\")\n",
    "    for category, percent in percentages.items():\n",
    "        print(f\"  {category}: {percent:.2f}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:27.525771Z",
     "iopub.status.busy": "2025-04-11T06:35:27.525325Z",
     "iopub.status.idle": "2025-04-11T06:35:27.589901Z",
     "shell.execute_reply": "2025-04-11T06:35:27.588909Z",
     "shell.execute_reply.started": "2025-04-11T06:35:27.525723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    frequency_table = fill_group_df[col].value_counts().reset_index()\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    frequency_table.columns = [col, 'Frequency']\n",
    "    \n",
    "    # Set the category names as the index\n",
    "    frequency_table.set_index(col, inplace=True)\n",
    "    \n",
    "    # Calculate the percentage for each category\n",
    "    frequency_table['Percentage'] = (frequency_table['Frequency'] / len(fill_group_df)) * 100\n",
    "    \n",
    "    # Print the frequency table\n",
    "    print(frequency_table)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Calculate the mode of the column\n",
    "    mode_value = fill_group_df[col].mode()[0]\n",
    "    \n",
    "    # Print the mode of the column\n",
    "    print(f\"Mode for Pollen_analysis: {mode_value}\\n\")\n",
    "    print(\"==\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:27.591320Z",
     "iopub.status.busy": "2025-04-11T06:35:27.591037Z",
     "iopub.status.idle": "2025-04-11T06:35:28.433291Z",
     "shell.execute_reply": "2025-04-11T06:35:28.432254Z",
     "shell.execute_reply.started": "2025-04-11T06:35:27.591293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in categorical_columns:\n",
    "    frequency_table = fill_group_df[col].value_counts().reset_index()\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    frequency_table.columns = [col, 'Frequency']\n",
    "    \n",
    "    # Set the category names as the index\n",
    "    frequency_table.set_index(col, inplace=True)\n",
    "    \n",
    "    # Calculate the percentage for each category\n",
    "    frequency_table['Percentage'] = (frequency_table['Frequency'] / len(fill_group_df)) * 100\n",
    "    \n",
    "    # Print the frequency table\n",
    "    print(frequency_table)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Calculate the mode of the column\n",
    "    mode_value = fill_group_df[col].mode()[0]\n",
    "    \n",
    "    # Print the mode of the column\n",
    "    print(f\"Mode for {col}: {mode_value}\\n\")\n",
    "    print(\"==\"*40)\n",
    "\n",
    "    # Create a barplot to visualize the frequency of each category\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=frequency_table.index, y=frequency_table['Frequency'], palette='viridis')\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title(f\"Category Frequency Distribution for {col}\", fontsize=14)\n",
    "    plt.xlabel(col, fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for readability (if needed)\n",
    "    plt.xticks(rotation=78)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Numerical Features Based on Robustness to Outliers\n",
    "\n",
    "Understanding how different metrics handle outliers is crucial in data analysis. Below are the basic metrics for numerical features, categorized by their robustness to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outliers are data points that are significantly different from other observations in the dataset. They can be a result of variability in the data or indicate measurement error. Identifying and handling outliers is crucial because they can greatly impact statistical analyses.\n",
    "\n",
    "- **Robust to Outliers**                  \n",
    "    - Median\n",
    "    - Interquartile Range (IQR)\n",
    "    - Median Absolute Deviation (MAD)\n",
    "    - Trimmed Mean\n",
    "    \n",
    "- **Sensitive to Outliers**                    \n",
    "    - Mean\n",
    "    - Standard Deviation\n",
    "    - Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:28.435668Z",
     "iopub.status.busy": "2025-04-11T06:35:28.435208Z",
     "iopub.status.idle": "2025-04-11T06:35:28.440537Z",
     "shell.execute_reply": "2025-04-11T06:35:28.439479Z",
     "shell.execute_reply.started": "2025-04-11T06:35:28.435611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List Comprehension for numeric features\n",
    "numerical_columns = [col for col in fill_group_df.columns if fill_group_df[col].dtypes in [\"float\",\"int\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:28.442128Z",
     "iopub.status.busy": "2025-04-11T06:35:28.441846Z",
     "iopub.status.idle": "2025-04-11T06:35:51.944734Z",
     "shell.execute_reply": "2025-04-11T06:35:51.943517Z",
     "shell.execute_reply.started": "2025-04-11T06:35:28.442101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plot_metrics(column_data, column_name, TARGET = \"Price\"):\n",
    "    # Creating subplots for different visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(column_data, kde=False, ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title(f\"Histogram of {column_name}\")\n",
    "    axes[0, 0].set_xlabel(column_name)\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(y=column_data, ax=axes[0, 1], color='lightgreen')\n",
    "    axes[0, 1].set_title(f\"Boxplot of {column_name}\")\n",
    "    axes[0, 1].set_xlabel(column_name)\n",
    "\n",
    "    # KDE (Kernel Density Estimation) plot\n",
    "    sns.scatterplot(x = column_data, y = fill_group_df[TARGET] ,ax=axes[1, 0], color='orange')\n",
    "    axes[1, 0].set_title(f\"Scatter plot of {column_name}\")\n",
    "    axes[1, 0].set_xlabel(column_name)\n",
    "    axes[1, 0].set_ylabel(\"Price\")\n",
    "\n",
    "    # Q-Q plot\n",
    "    stats.probplot(column_data, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title(f\"Q-Q plot of {column_name}\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming numerical_columns and fill_group_df are already defined\n",
    "for col in numerical_columns:\n",
    "    col_data = fill_group_df[col]\n",
    "    \n",
    "    # Plot the graphs for each column\n",
    "    plot_metrics(col_data, col)\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:51.947046Z",
     "iopub.status.busy": "2025-04-11T06:35:51.946482Z",
     "iopub.status.idle": "2025-04-11T06:35:51.955314Z",
     "shell.execute_reply": "2025-04-11T06:35:51.954164Z",
     "shell.execute_reply.started": "2025-04-11T06:35:51.946998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def drop_outlier(df: pd.DataFrame, columns_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes rows containing outliers in specified columns from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "    columns_list (list): List of column names to check for outliers.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    print(\"Old data shape: \",data.shape) \n",
    "    \n",
    "    for col in columns_list:\n",
    "        q1 = data[col].quantile(0.25)\n",
    "        q3 = data[col].quantile(0.75)\n",
    "        IQR = q3 - q1\n",
    "        fence_low = q1 - (1.5 * IQR)\n",
    "        fence_high = q3 + (1.5 * IQR)\n",
    "        \n",
    "        # Filter rows where column values are within the fence\n",
    "        data = data[(data[col] >= fence_low) & (data[col] <= fence_high)]\n",
    "\n",
    "    print(\"New data shape: \",data.shape) \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:51.956934Z",
     "iopub.status.busy": "2025-04-11T06:35:51.956590Z",
     "iopub.status.idle": "2025-04-11T06:35:52.164065Z",
     "shell.execute_reply": "2025-04-11T06:35:52.163008Z",
     "shell.execute_reply.started": "2025-04-11T06:35:51.956904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "no_outlier_data = drop_outlier(fill_group_df,numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Screening\n",
    "\n",
    "Feature screening is a crucial step in the data quality process that involves identifying and removing features (variables) that do not contribute meaningful information to the analysis or modeling. By eliminating such features, we can streamline the dataset, improve model performance, and enhance interpretability. In this section, we will discuss three specific criteria for feature screening:\n",
    "\n",
    "**Features with Coefficient of Variation Less than 0.1 for Continuous Variables**\n",
    "\n",
    "The coefficient of variation (CV) is a measure of relative variability. It is calculated as the ratio of the standard deviation to the mean. Features with a CV of less than 0.1 are considered to have low variability and may not provide important information for analysis. We will identify and remove such features.\n",
    "\n",
    "**Features with Mode Category Percentage Greater than 95% for Categorical Variables**\n",
    "\n",
    "Categorical variables dominated by a single category (mode category percentage > 95%) may not be useful for analysis because they do not provide much variation. We will identify and remove these categorical features to streamline the dataset.\n",
    "\n",
    "**Features with a Percentage of Unique Categories Exceeding 90% for Categorical Variables**\n",
    "\n",
    "Categorical variables with a high percentage of unique categories (>90%) can complicate analysis and lead to overfitting of models. We will identify and remove these features to provide a more robust and generalizable model.\n",
    "\n",
    "By applying these elimination criteria, we can ensure that the features remaining in the dataset provide meaningful and relevant information for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.165682Z",
     "iopub.status.busy": "2025-04-11T06:35:52.165336Z",
     "iopub.status.idle": "2025-04-11T06:35:52.175882Z",
     "shell.execute_reply": "2025-04-11T06:35:52.174742Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.165650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_screening(df: pd.DataFrame,\n",
    "                      target: str, \n",
    "                      categorical_columns: list, \n",
    "                      numerical_columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs feature screening on a dataset by analyzing numerical and categorical features \n",
    "    to identify and remove less informative or redundant features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset containing features and the target variable.\n",
    "\n",
    "    target : str\n",
    "        The name of the target variable column.\n",
    "\n",
    "    categorical_columns : list\n",
    "        A list of column names corresponding to categorical features.\n",
    "\n",
    "    numerical_columns : list\n",
    "        A list of column names corresponding to numerical features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A cleaned dataset with the identified features removed.\n",
    "\n",
    "    Methodology:\n",
    "    ------------\n",
    "    - For numerical features:\n",
    "      - Calculates the Coefficient of Variation (CV) and identifies features with CV < 0.1.\n",
    "\n",
    "    - For categorical features:\n",
    "      - Identifies features where the most frequent category (mode) exceeds 95% of the data.\n",
    "      - Identifies features where the percentage of unique categories exceeds 90%.\n",
    "\n",
    "    - Combines all identified features to be removed and drops them from the dataset.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {\n",
    "    ...     'num1': [1, 1, 1, 1],\n",
    "    ...     'num2': [2, 2, 2, 2],\n",
    "    ...     'cat1': ['A', 'A', 'A', 'A'],\n",
    "    ...     'cat2': ['B', 'C', 'D', 'E'],\n",
    "    ...     'target': [0, 1, 0, 1]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> cleaned_data = feature_screening(\n",
    "    ...     df=df, \n",
    "    ...     target='target', \n",
    "    ...     categorical_columns=['cat1', 'cat2'], \n",
    "    ...     numerical_columns=['num1', 'num2']\n",
    "    ... )\n",
    "    >>> print(cleaned_data)\n",
    "    \"\"\"\n",
    "    # Separate the dataset into input variables (predictors) and target variable (response)\n",
    "    label = df[target]\n",
    "    inputs = df.drop(columns=[target])\n",
    "    \n",
    "    # Calculate Coefficient of Variation for continuous variables\n",
    "    cv = inputs[numerical_columns[:-1]].std() / inputs[numerical_columns[:-1]].mean()\n",
    "    \n",
    "    # Identify features with CV less than 0.1\n",
    "    low_cv_features = cv[cv < 0.1].index.tolist()\n",
    "    print(\"Features with Coefficient of Variation less than 0.1:\", low_cv_features)\n",
    "    \n",
    "    # Calculate Mode Category Percentage for categorical variables\n",
    "    mode_percentage = inputs[categorical_columns].apply(lambda x: x.value_counts(normalize=True).max() * 100)\n",
    "    \n",
    "    # Identify features where the mode category percentage is greater than 95%\n",
    "    high_mode_features = mode_percentage[mode_percentage > 95].index.tolist()\n",
    "    print(\"Categorical features where mode category percentage is greater than 95%:\", high_mode_features)\n",
    "    \n",
    "    # Calculate Percentage of Unique Categories for categorical variables\n",
    "    unique_category_percentage = inputs[categorical_columns].nunique() / len(inputs) * 100\n",
    "    \n",
    "    # Identify features with a percentage of unique categories exceeding 90%\n",
    "    high_unique_features = unique_category_percentage[unique_category_percentage > 90].index.tolist()\n",
    "    print(\"Categorical features with percentage of unique categories exceeding 90%:\", high_unique_features)\n",
    "    \n",
    "    # Combine all features to be removed\n",
    "    features_to_remove = set(low_cv_features + high_mode_features + high_unique_features)\n",
    "    print(\"Features to be removed:\", features_to_remove)\n",
    "    \n",
    "    # Remove the identified features from the inputs dataframe\n",
    "    cleaned_inputs = inputs.drop(columns=features_to_remove)\n",
    "    \n",
    "    # Combine the cleaned inputs with the label\n",
    "    cleaned_dataset = pd.concat([cleaned_inputs, label], axis=1)\n",
    "    \n",
    "    # Display the cleaned dataset\n",
    "    return cleaned_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.177646Z",
     "iopub.status.busy": "2025-04-11T06:35:52.177294Z",
     "iopub.status.idle": "2025-04-11T06:35:52.316091Z",
     "shell.execute_reply": "2025-04-11T06:35:52.314774Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.177613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cleaned_dataset = feature_screening(no_outlier_data,target = \"Price\",\n",
    "                      numerical_columns = numerical_columns,\n",
    "                      categorical_columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.318230Z",
     "iopub.status.busy": "2025-04-11T06:35:52.317761Z",
     "iopub.status.idle": "2025-04-11T06:35:52.323598Z",
     "shell.execute_reply": "2025-04-11T06:35:52.322498Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.318182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numerical_columns.remove(\"EC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.325614Z",
     "iopub.status.busy": "2025-04-11T06:35:52.324956Z",
     "iopub.status.idle": "2025-04-11T06:35:52.333953Z",
     "shell.execute_reply": "2025-04-11T06:35:52.332894Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.325572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    # G (Glucose Level) / Purity:\n",
    "    # Measures the proportion of glucose relative to the purity. Higher values may indicate adulteration or lower quality.\n",
    "    df[\"pur_G_ratio\"] = df[\"G\"] / df[\"Purity\"]\n",
    "    \n",
    "    # F (Fructose Level) / Purity:\n",
    "    # Measures the proportion of fructose relative to purity. Natural honey tends to have more fructose than glucose.\n",
    "    df[\"pur_F_ratio\"] = df[\"F\"] / df[\"Purity\"]\n",
    "    \n",
    "    # WC (Water Content) / Purity:\n",
    "    # Indicates the amount of water in relation to the purity. Excess water may suggest poor storage or added content.\n",
    "    df[\"pur_WC_ratio\"] = df[\"WC\"] / df[\"Purity\"]\n",
    "    \n",
    "    # (F + G) / Purity:\n",
    "    # Represents the total sugar (fructose + glucose) content normalized by purity. Very high values may indicate sugar adulteration.\n",
    "    df[\"pur_FG_ratio\"] = (df[\"F\"] + df[\"G\"]) / df[\"Purity\"]\n",
    "    \n",
    "    # CS (Color Score) / Purity:\n",
    "    # Color can reflect the floral source and processing. This ratio helps explore how color relates to purity.\n",
    "    df[\"pur_CS_ratio\"] = df[\"CS\"] / df[\"Purity\"]\n",
    "    \n",
    "    # (Density * Viscosity) / Purity:\n",
    "    # Combines two physical characteristics to analyze their joint effect on purity. Dense and viscous honey is usually higher quality.\n",
    "    df[\"den_Vis_ratio\"] = (df[\"Density\"] * df[\"Viscosity\"]) / df[\"Purity\"]\n",
    "    \n",
    "    # (F² + G²) / Viscosity:\n",
    "    # A non-linear combination of sugar content compared to viscosity. Helps assess how sugar concentration affects thickness.\n",
    "    df[\"vis_FG_ratio\"] = ((df[\"F\"]**2) + (df[\"G\"]**2)) / df[\"Viscosity\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.335674Z",
     "iopub.status.busy": "2025-04-11T06:35:52.335263Z",
     "iopub.status.idle": "2025-04-11T06:35:52.376165Z",
     "shell.execute_reply": "2025-04-11T06:35:52.375143Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.335630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df =feature_extraction(cleaned_dataset)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Virtualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:52.377776Z",
     "iopub.status.busy": "2025-04-11T06:35:52.377411Z",
     "iopub.status.idle": "2025-04-11T06:35:53.593334Z",
     "shell.execute_reply": "2025-04-11T06:35:53.592191Z",
     "shell.execute_reply.started": "2025-04-11T06:35:52.377744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Korelasyon matrisi hesaplama\n",
    "corr_matrix = new_df.corr(numeric_only=True)\n",
    "\n",
    "# Üst üçgen için maske oluşturma\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap oluşturma\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    mask=mask, \n",
    "    annot=True,  # Korelasyon değerlerini göster\n",
    "    fmt=\".3f\",  # Değer formatı\n",
    "    cmap='coolwarm',  # Renk skalası\n",
    "    vmin=-1, vmax=1,  # Korelasyon aralığı\n",
    "    linewidths=0.5,  # Hücre ayracı genişliği\n",
    "    cbar_kws={'shrink': 0.8}  # Renk çubuğu boyutu\n",
    ")\n",
    "\n",
    "plt.title('Korelasyon Matrisi (Üst Üçgen Gizlenmiş)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:53.595558Z",
     "iopub.status.busy": "2025-04-11T06:35:53.595129Z",
     "iopub.status.idle": "2025-04-11T06:35:54.167342Z",
     "shell.execute_reply": "2025-04-11T06:35:54.166259Z",
     "shell.execute_reply.started": "2025-04-11T06:35:53.595514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate correlations between one variable and others (e.g. target variable: 'target')\n",
    "correlations = new_df.corr(numeric_only=True)['Price'].drop('Price')\n",
    "\n",
    "# Çubuk grafiği\n",
    "sns.barplot(x=correlations.values, y=correlations.index, palette='viridis')\n",
    "plt.title(\"Correlation with Target Variable\")\n",
    "plt.xlabel(\"Correlation Coefficient\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:54.169121Z",
     "iopub.status.busy": "2025-04-11T06:35:54.168804Z",
     "iopub.status.idle": "2025-04-11T06:35:54.174545Z",
     "shell.execute_reply": "2025-04-11T06:35:54.173474Z",
     "shell.execute_reply.started": "2025-04-11T06:35:54.169080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List Comprehension for numeric features\n",
    "numerical_columns = [col for col in new_df.columns if new_df[col].dtypes in [\"float\",\"int\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:54.176209Z",
     "iopub.status.busy": "2025-04-11T06:35:54.175866Z",
     "iopub.status.idle": "2025-04-11T06:35:59.822840Z",
     "shell.execute_reply": "2025-04-11T06:35:59.821512Z",
     "shell.execute_reply.started": "2025-04-11T06:35:54.176177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4,figsize=(15,12))\n",
    "axes = ax.flatten()\n",
    "for i,col in enumerate(numerical_columns):\n",
    "    sns.histplot(data=cleaned_dataset , x=col, stat=\"frequency\", ax=axes[i])\n",
    "\n",
    "\n",
    "# Kullanılmayan eksenleri kapat\n",
    "for j in range(len(numerical_columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **EC** and **Purity** columns contain descrete data.\n",
    "* For the **Purity** column (low_purity, normal, high_purity), the **cat_purity** column can be created.\n",
    "* For the **EC** column (low_ec, normal, high_ec), the **cat_ec** column can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of numerical variables according to the Price variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:35:59.824754Z",
     "iopub.status.busy": "2025-04-11T06:35:59.824278Z",
     "iopub.status.idle": "2025-04-11T06:36:10.586345Z",
     "shell.execute_reply": "2025-04-11T06:36:10.584833Z",
     "shell.execute_reply.started": "2025-04-11T06:35:59.824715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,4,figsize=(15,12))\n",
    "axes = ax.flatten()\n",
    "\n",
    "for i,col in enumerate(numerical_columns):\n",
    "    sns.scatterplot(data=new_df , x=col, y=\"Price\" ,ax=axes[i])\n",
    "\n",
    "\n",
    "# REMOVE unused axes\n",
    "for j in range(len(numerical_columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Pollen_analysis variable according to Price variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:10.588705Z",
     "iopub.status.busy": "2025-04-11T06:36:10.587978Z",
     "iopub.status.idle": "2025-04-11T06:36:11.547894Z",
     "shell.execute_reply": "2025-04-11T06:36:11.546856Z",
     "shell.execute_reply.started": "2025-04-11T06:36:10.588658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=new_df , x=\"Pollen_analysis\", y=\"Price\",color=\"skyblue\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Category Differences:**\n",
    "\n",
    "* There are differences in price ranges between categories.\n",
    "- For example:\n",
    "- The \"Manuka\" category seems to be more concentrated in the higher price range compared to other categories.\n",
    "- Prices in the \"Clover\" and \"Thyme\" categories are generally concentrated in the middle range.\n",
    "- Prices in categories such as \"Acacia\" and \"Tupelo\" are observed in a narrower range.\n",
    "\n",
    "**Price Concentration:**\n",
    "\n",
    "- In many categories, price values ​​are concentrated between 400-600 units.\n",
    "- However, in some categories, there are lower or higher price values.\n",
    "- It may mean that some honey categories are sold at higher prices. However, we cannot say this by just looking at this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:11.549583Z",
     "iopub.status.busy": "2025-04-11T06:36:11.549186Z",
     "iopub.status.idle": "2025-04-11T06:36:11.994145Z",
     "shell.execute_reply": "2025-04-11T06:36:11.993070Z",
     "shell.execute_reply.started": "2025-04-11T06:36:11.549548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pollen_price = new_df.groupby(\"Pollen_analysis\")[\"Price\"].mean().sort_values(ascending=False)\n",
    "sns.barplot(y=pollen_price.index, x=pollen_price.values)\n",
    "plt.title(\"Average Sales Prices by Pollen Types\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Categorical Variables to Numerical Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will apply Label encoding to **Purity_group** and **ph_group** columns. Because there is a sorting between them. (For example: low,medium,normal)\n",
    "- We will apply One Hot Encoding to **Pollen_analysis** column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:11.995955Z",
     "iopub.status.busy": "2025-04-11T06:36:11.995552Z",
     "iopub.status.idle": "2025-04-11T06:36:12.000751Z",
     "shell.execute_reply": "2025-04-11T06:36:11.999774Z",
     "shell.execute_reply.started": "2025-04-11T06:36:11.995910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# new_df[\"Purity_group\"] = new_df[\"Purity_group\"].replace({\"Low\":0,\n",
    "#                              \"Medium\":1,\n",
    "#                              \"High\":2})\n",
    "\n",
    "# new_df[\"pH_group\"] = new_df[\"pH_group\"].replace({\"Low\":0,\n",
    "#                              \"Medium\":1,\n",
    "#                              \"Normal\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do one hot encoding using the **pd.get_dummies()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:12.002325Z",
     "iopub.status.busy": "2025-04-11T06:36:12.001908Z",
     "iopub.status.idle": "2025-04-11T06:36:12.012615Z",
     "shell.execute_reply": "2025-04-11T06:36:12.011525Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.002292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dummies_df = pd.get_dummies(new_df[\"Pollen_analysis\"],drop_first=True,dtype=\"int\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:12.014580Z",
     "iopub.status.busy": "2025-04-11T06:36:12.014123Z",
     "iopub.status.idle": "2025-04-11T06:36:12.023812Z",
     "shell.execute_reply": "2025-04-11T06:36:12.022823Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.014533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_data = pd.concat([new_df,dummies_df],axis=1).drop(columns=\"Pollen_analysis\")\n",
    "# print(\"Model dataset shape: \",model_data.shape)\n",
    "# model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Separation of Feature and Label variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T06:36:12.025337Z",
     "iopub.status.busy": "2025-04-11T06:36:12.024977Z",
     "iopub.status.idle": "2025-04-11T06:36:12.359318Z",
     "shell.execute_reply": "2025-04-11T06:36:12.357787Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.025304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = model_data.drop(columns=\"Price\")\n",
    "y = model_data[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.360398Z",
     "iopub.status.idle": "2025-04-11T06:36:12.360810Z",
     "shell.execute_reply": "2025-04-11T06:36:12.360655Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.360636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cats = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "X[cats] = X[cats].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.362119Z",
     "iopub.status.idle": "2025-04-11T06:36:12.362537Z",
     "shell.execute_reply": "2025-04-11T06:36:12.362318Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.362302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.364587Z",
     "iopub.status.idle": "2025-04-11T06:36:12.365133Z",
     "shell.execute_reply": "2025-04-11T06:36:12.364896Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.364868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.366251Z",
     "iopub.status.idle": "2025-04-11T06:36:12.366797Z",
     "shell.execute_reply": "2025-04-11T06:36:12.366554Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.366527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the objective function for hyperparameter optimization\n",
    "# def objective(trial):\n",
    "#     # Hyperparameters to be tuned using Optuna\n",
    "#     param = {\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'eval_metric': 'rmse',\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9),  # Proportion of features used for trees\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),  # Learning rate\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),  # Depth of the tree\n",
    "#         'alpha': trial.suggest_float('alpha', 0.01, 1),  # L2 regularization term\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 600),  # Number of trees\n",
    "#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),  # Proportion of data used for training\n",
    "#     }\n",
    "    \n",
    "#     # Set up KFold for k-fold cross-validation (e.g., 5 folds)\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "#     # List to store the performance (RMSE) for each fold\n",
    "#     cv_rmse_scores = []\n",
    "    \n",
    "#     # KFold loop\n",
    "#     for train_index, val_index in kf.split(X_train):\n",
    "#         # Split data into training and validation sets\n",
    "#         train_x, valid_x = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#         train_y, valid_y = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "#         # Create XGBoost model\n",
    "#         model = xgb.XGBRegressor(**param,enable_categorical=True)\n",
    "\n",
    "#         # Train the model on the training data\n",
    "#         model.fit(train_x, train_y)\n",
    "\n",
    "#         # Make predictions on the validation set\n",
    "#         y_pred = model.predict(valid_x)\n",
    "\n",
    "#         # Calculate RMSE and append it to the list\n",
    "#         rmse = mean_squared_error(valid_y, y_pred, squared=False)  # squared=False returns RMSE\n",
    "#         cv_rmse_scores.append(rmse)\n",
    "    \n",
    "#     # Return the average RMSE from all folds\n",
    "#     mean_rmse = np.mean(cv_rmse_scores)\n",
    "#     return mean_rmse  # Optuna will try to minimize this value\n",
    "\n",
    "# # Start the Optuna optimization\n",
    "# study = optuna.create_study(direction='minimize')  # Goal is to minimize RMSE\n",
    "# study.optimize(objective, n_trials=50)  # 50 trials will be performed\n",
    "\n",
    "# # Print the best parameters and the lowest RMSE found\n",
    "# print(\"Best parameters: \", study.best_params)\n",
    "# print(\"Lowest RMSE: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.368799Z",
     "iopub.status.idle": "2025-04-11T06:36:12.369324Z",
     "shell.execute_reply": "2025-04-11T06:36:12.369080Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.369053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the final model with the best parameters\n",
    "best_params = {'colsample_bytree': 0.8841336691493017, 'learning_rate':\n",
    "               0.4951441846210324, 'max_depth': 5, 'alpha': 0.8754677850368304, 'n_estimators': 598, 'subsample': 0.8728936635344561}\n",
    "best_model = xgb.XGBRegressor(**best_params,enable_categorical=True)\n",
    "\n",
    "# Retrain the model on the full data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE) on the test set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.370763Z",
     "iopub.status.idle": "2025-04-11T06:36:12.371126Z",
     "shell.execute_reply": "2025-04-11T06:36:12.370972Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.370954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Let's visualize Actual vs Predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)\n",
    "plt.xlabel('Gerçek Değerler')\n",
    "plt.ylabel('Tahmin Edilen Değerler')\n",
    "plt.title('Gerçek Değerler vs Tahmin Edilen Değerler')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-11T06:36:12.372327Z",
     "iopub.status.idle": "2025-04-11T06:36:12.372705Z",
     "shell.execute_reply": "2025-04-11T06:36:12.372547Z",
     "shell.execute_reply.started": "2025-04-11T06:36:12.372529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(best_model, 'xgboost_model.pkl')\n",
    "\n",
    "# Let's load the saved model again\n",
    "loaded_model = joblib.load('xgboost_model.pkl')\n",
    "\n",
    "# Let's predict on the test data\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "print(f\"Yüklenen modelin R-kare skoru: {r2_score(y_test, y_pred_loaded):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4458973,
     "sourceId": 7649296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
